# Ryumem Environment Variables
# Copy this file to .env and fill in your actual values

# OpenAI API Configuration
# Required for embeddings, and required for LLM if using llm_provider='openai'
OPENAI_API_KEY=your_openai_api_key_here

# LLM Provider Configuration
# Choose between 'openai' (cloud API) or 'ollama' (local inference)
# RYUMEM_LLM_PROVIDER=openai  # Default: openai

# OpenAI LLM Configuration (when llm_provider='openai')
# RYUMEM_LLM_MODEL=gpt-4  # Default: gpt-4

# Ollama LLM Configuration (when llm_provider='ollama')
# Make sure Ollama is running: ollama serve
# Pull a model first: ollama pull llama3.2:3b
# RYUMEM_LLM_PROVIDER=ollama
# RYUMEM_LLM_MODEL=llama3.2:3b  # Recommended: llama3.2:3b, mistral:7b, qwen2.5:7b
# RYUMEM_OLLAMA_BASE_URL=http://localhost:11434  # Default Ollama URL

# Embedding Model Configuration
# RYUMEM_EMBEDDING_MODEL=text-embedding-3-large  # Default: text-embedding-3-large

# Optional: Database path (default: ./data/ryumem.db)
# RYUMEM_DB_PATH=./data/ryumem.db

# Optional: Logging level (DEBUG, INFO, WARNING, ERROR)
# LOG_LEVEL=INFO
